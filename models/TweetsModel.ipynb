{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbJ-iy8o3gh-"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY6rNvMZ4ah7"
      },
      "source": [
        "# All libraries\n",
        "import re\n",
        "import os\n",
        "import logging\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import RobertaTokenizerFast, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Embedding, Reshape, Flatten, Dropout, GRU, Dense, RepeatVector, Dense, Activation, Lambda, Softmax, Conv1D, LayerNormalization, Softmax, Multiply\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.models import load_model, Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping ,LearningRateScheduler\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ9AH4tP4j2A"
      },
      "source": [
        "class Model():\n",
        "\n",
        "  def __init__(self, pretrained_name=\"roberta-base\"):\n",
        "    # Choose the pretrained model to use, such as \"roberta-base\", \"roberta-large\", etc.\n",
        "    self.pretrained_name = pretrained_name\n",
        "\n",
        "\n",
        "  # This function output (features, labels) of a dataset.\n",
        "  # Each element in 'features' is an array represent one specific feature of all datapoints in the dataset. \n",
        "  # Same for 'labels'.\n",
        "  def process_df(self, dataframe, label_names):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for key in list(dataframe.keys()):\n",
        "      if key in label_names:\n",
        "        labels = labels.append(np.array(dataframe[key]))\n",
        "      else:\n",
        "        features = features.appned(np.array(dataframe[key]))\n",
        "    return (features, labels)\n",
        "\n",
        "\n",
        "  # Load pretrained models from Hugging Face: \n",
        "  def load_pretrained(self):\n",
        "    # 1. Pretrained tokenizer (to turn text into numerical inputs)\n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained(self.pretrained_name)\n",
        "\n",
        "    # 2. Pretrained BERT (takes in the tokenized inputs and predicts)\n",
        "    roberta = TFRobertaModel.from_pretrained(self.pretrained_name)\n",
        "\n",
        "    return tokenizer, roberta\n",
        "  \n",
        "\n",
        "  # Produce numerical outputs for texts\n",
        "  def tokenize_texts(self, texts, tokenizer, max_tweet_len):\n",
        "    # 1 text can be tokenized in many ways, hence, tokenizer.batch_encode_plus outputs a dictionary result \n",
        "    tokenized = tokenizer.batch_encode_plus(texts.tolist(), padding='max_length', max_length=max_tweet_len, truncation=True, return_tensors='np')\n",
        "\n",
        "    # We will choose 2 of the tokenized results for learning\n",
        "    return tokenized['input_ids'], tokenized['attention_mask']\n",
        "\n",
        "\n",
        "  # The rough template for BERT, add more layer to fine-tune the model\n",
        "  def build_model(self, featrues, labels, max_tweet_len, learning_rate, decay):\n",
        "    inputs = []\n",
        "    outputs = []\n",
        "    # Inputs: IDs and Attention Mask\n",
        "    ids = Input(shape=(max_tweet_len,), dtype='int64')\n",
        "    mask = Input(shape=(max_tweet_len,), dtype='int64')\n",
        "    inputs = inputs.append([ids, mask])\n",
        "\n",
        "    for i in range(len(features)):\n",
        "      ft = Input(shape=(1,))\n",
        "      inputs = inputs.append(ft)\n",
        "    \n",
        "    # Input goes through pretrained BERT\n",
        "    x = roberta(input_ids=ids, attention_mask=mask)['last_hidden_state']\n",
        "    # We use only the classification token of the output\n",
        "    x = x[:, 0, :]\n",
        "    # Regularization. Drop rate ([0, 1]) is usually shared among many layers\n",
        "    drop_rate = 0.2 \n",
        "    x = Dropout(drop_rate)(x)\n",
        "\n",
        "    # Fine-tuning\n",
        "    # Add more layers on top of BERT here: ...\n",
        "    # Example: z = Dense(10, ...)(inputs[2])\n",
        "\n",
        "    # Final output here, change it to whatever we want to predict\n",
        "    prediction_1 = Dense(1, activation='linear')(x)\n",
        "    # Append all the output as specified in 'labels'\n",
        "    outputs = outputs.append(prediction_1)\n",
        "    # Examples: outputs = outputs.append(prediction_2)...\n",
        "    \n",
        "    # Produce model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer = Adam(learning_rate=learning_rate, decay=decay), loss = [MeanSquaredError()], metrics = [RootMeanSquaredError()])\n",
        "    return model\n",
        "\n",
        "\n",
        "  # Function to seed randomness\n",
        "  def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "  # Combine all the previous step and train the model\n",
        "  def train_model(dataframe, label_names, max_tweet_len, learning_rate, decay, batch, epoch_size, cp_path, seed):\n",
        "    # Process information from dataframe\n",
        "    features, labels = process_df(dataframe, label_names)\n",
        "\n",
        "    # Create model\n",
        "    model = build_model(featrues, labels, max_tweet_len, learning_rate, decay)\n",
        "    model.summary()\n",
        "      \n",
        "    # Seed \n",
        "    seed_everything(seed)\n",
        "\n",
        "    # Checkpoints for training\n",
        "    checkpoint = ModelCheckpoint(cp_path, monitor = \"val_root_mean_squared_error\", verbose = 2, save_best_only = True, save_weights_only = True, mode = 'min')\n",
        "    reduce_lr = ReduceLROnPlateau(monitor=\"val_root_mean_squared_error\", factor=0.8, patience=5, min_lr=1e-8)\n",
        "    early_stopping = EarlyStopping(monitor=\"val_root_mean_squared_error\", min_delta=0, patience=5, verbose=2, mode=\"min\", restore_best_weights=True)\n",
        "\n",
        "    # Use 5-fold CV\n",
        "    kfold = KFold(n_splits = 5, shuffle = True, random_state = seed)\n",
        "    \n",
        "    for fold, (train_indexes, val_indexes) in enumerate(kfold.split(len(features[0]))):\n",
        "      # Verbosity\n",
        "      print('\\nFold', fold+1, '*'*50)\n",
        "\n",
        "      # Extract train/validation sets\n",
        "      train_features = []\n",
        "      val_features = []\n",
        "      train_labels = []\n",
        "      val_labels = []\n",
        "      for ft in features:\n",
        "        train_features = train_features.append(ft[train_indexes])\n",
        "        val_features = val_features.append(ft[val_indexes])\n",
        "      for lb in labels:\n",
        "        train_labels = train_labels.append(ft[train_indexes])\n",
        "        val_labels = val_labels.append(ft[val_indexes])\n",
        "\n",
        "      # Training and Validating\n",
        "      try:\n",
        "        model.load_weights(cp_path + str(fold+1))\n",
        "      except:\n",
        "        pass\n",
        "      model.fit((train_features, train_labels), batch_size=batch, epochs=epoch_size, validation_data = (val_features, val_labels), callbacks = [checkpoint, reduce_lr, early_stopping])\n",
        "      model.load_weights(cp_path + str(fold+1))\n",
        "   \n",
        "    return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtfR-WgG4pOB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}